# Spatial BERT configuration for dual-task learning

model:
  arch: bert
  vocab_size: 60000
  n_genes: 500
  d_model: 96         
  n_layers: 3       
  n_heads: 3          
  dropout: 0.1       
  expr_mask_ratio: 0.2
  pos_mask_ratio: 0.2  
  coord_noise_std: 10.0 
  max_value: 512
  padding_idx: 0

dataset:
  data_dir: "/leonardo_work/EUHPC_B25_011/ST/DLPFC"
  n_spots: 512
  n_hvg: 500         
  use_hvg: true
  sampling_method: "nearest"
  normalize_total: 10000.0
  log1p: true
  max_gene_len: 500 
  patches_per_slide: 50  
  batch_size: 4
  num_workers: 4

tokenizer:
  vocab_file: null
  default_vocab_type: "census"
  special_tokens: ["<pad>", "<unk>", "<mask>", "<cls>", "<eos>"]
  default_token: "<pad>"

training:
  learning_rate: 0.0005  
  weight_decay: 0.01     
  warmup_steps: 100      
  max_epochs: 100
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  
  # BERT loss weights
  expr_weight: 1.0        
  coord_weight: 0.0001    
  
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_eps: 1.0e-8
  
  scheduler: "cosine"
  min_lr: 1.0e-6
  t_max: 500
  
  early_stopping: true
  patience: 20
  min_delta: 0.0001
  
logging:
  project_name: "spatial-bert"
  run_name: "dual_task_learning"
  logger: "wandb"
  log_dir: "./logs"
  save_dir: "./checkpoints"
  log_every_n_steps: 10
  val_check_interval: 0.5
  save_top_k: 3
  monitor_metric: "val_loss"
  monitor_mode: "min"

compute:
  accelerator: "gpu"
  devices: 1
  precision: "32"  
  strategy: "auto"
  gradient_checkpointing: false
  compile_model: false
  detect_anomaly: false
  enable_cpu_offload: false
  empty_cache_freq: 100

seed: 42
debug: false
resume_from_checkpoint: null
test_only: false
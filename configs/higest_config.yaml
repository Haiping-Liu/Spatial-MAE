# HiGeST configuration - Hierarchical Gene expression in Spatial Transcriptomics

model:
  arch: higest
  vocab_size: 60000
  n_genes: 300
  d_model: 96         
  n_layers: 3       
  n_heads: 3          
  dropout: 0.1       
  gene_mask_ratio: 0.15   # Mask 15% of valid genes per spot
  noise_scale: 0.1        # Noise scale for masked genes
  
  # Topology learning parameters
  k_neighbors: 6          # Number of nearest neighbors for adjacency matrix
  lambda_topo: 0.1        # Fixed weight for topology loss
  ema_decay: 0.995        # Fixed EMA decay for teacher update
  max_value: 512
  padding_idx: 0

dataset:
  data_dir: "/leonardo_work/EUHPC_B25_011/ST/DLPFC"
  n_spots: 512
  n_hvg: 300         
  use_hvg: true
  sampling_method: "nearest"
  normalize_total: 10000.0
  log1p: true
  max_gene_len: 300 
  patches_per_slide: 50  
  batch_size: 4
  num_workers: 4

tokenizer:
  vocab_file: null
  default_vocab_type: "census"
  special_tokens: ["<pad>", "<unk>", "<mask>", "<cls>", "<eos>"]
  default_token: "<pad>"

training:
  learning_rate: 0.0005  
  weight_decay: 0.01     
  warmup_epochs: 10      
  max_epochs: 100
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_eps: 1.0e-8
  
  scheduler: "cosine"
  min_lr: 1.0e-6
  t_max: 500
  
  early_stopping: true
  patience: 20
  min_delta: 0.0001
  
logging:
  project_name: "HiGeST"
  run_name: "higest_training"
  logger: "tensorboard"
  log_dir: "./logs"
  save_dir: "./checkpoints"
  log_every_n_steps: 10
  val_check_interval: 0.5
  save_top_k: 3
  monitor_metric: "val_loss"
  monitor_mode: "min"

compute:
  accelerator: "gpu"
  devices: 1
  precision: "32"  
  strategy: "auto"
  gradient_checkpointing: false
  compile_model: false
  detect_anomaly: false
  enable_cpu_offload: false
  empty_cache_freq: 100

seed: 42
debug: false
resume_from_checkpoint: null
test_only: false

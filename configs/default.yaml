# Default configuration for Spatial MAE - Small model (~1M params)

model:
  vocab_size: 60000  # Will be updated based on tokenizer vocabulary
  n_genes: 1000      # Will be updated based on max_gene_len
  d_model: 96        # 小维度
  d_decoder: 48      # 小解码器
  n_encoder_layers: 4  # 4层编码器
  n_decoder_layers: 2  # 2层解码器
  n_heads: 4         # 3个头，每个32维
  dropout: 0.1      # 强dropout防止过拟合
  mask_ratio: 0.75    # 低mask比例，更容易学习
  max_value: 512
  padding_idx: 0     # Will be updated based on tokenizer

dataset:
  data_dir: "/leonardo_work/EUHPC_B25_011/ST/DLPFC"
  n_spots: 128
  n_hvg: 1000
  use_hvg: true
  sampling_method: "nearest"  # "nearest" or "random"
  normalize_total: 10000.0
  log1p: true
  max_gene_len: 1000
  patches_per_slide: 200  # 适度增加patches
  batch_size: 8  # 保持小batch避免OOM
  num_workers: 4
  train_ratio: 0.8
  val_ratio: 0.1


tokenizer:
  vocab_file: null  # Path to custom vocab file, null to use default
  default_vocab_type: "census"  # "census" or "standard"
  special_tokens: ["<pad>", "<unk>", "<mask>", "<cls>", "<eos>"]
  default_token: "<pad>"

training:
  learning_rate: 0.0003
  weight_decay: 0.1  # 非常强的正则化
  warmup_steps: 100  # 减少warmup
  max_epochs: 100
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  
  # Optimizer
  optimizer: "adamw"  # "adamw", "adam", "sgd"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_eps: 1.0e-8
  
  # Scheduler
  scheduler: "cosine"  # "cosine", "reduce_on_plateau", "onecycle", null
  min_lr: 1.0e-6
  t_max: 4000  # 适配更多patches
  
  # Early stopping
  early_stopping: true
  patience: 10
  min_delta: 0.0001
  
  # Loss
  loss_type: "mse"  # "mse", "mae", "huber"
  huber_delta: 1.0

logging:
  project_name: "spatial-mae"
  run_name: null  # Auto-generated if null
  logger: "wandb"  # "wandb", "tensorboard", null
  log_dir: "./logs"
  save_dir: "./checkpoints"
  log_every_n_steps: 10
  val_check_interval: 1.0
  save_top_k: 3
  monitor_metric: "val_loss"  # 监控验证集防止过拟合
  monitor_mode: "min"

compute:
  accelerator: "gpu"
  devices: 1  # Number of GPUs
  precision: "16"  # "32", "16-mixed", "bf16-mixed"
  strategy: "auto"  # "auto", "ddp", "dp"
  gradient_checkpointing: false
  compile_model: false
  detect_anomaly: false
  
  # Memory optimization
  enable_cpu_offload: false
  empty_cache_freq: 100

# Runtime
seed: 42
debug: false
resume_from_checkpoint: null
test_only: false